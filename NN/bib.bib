
@article{fetsch_neural_2012,
	title = {Neural correlates of reliability-based cue weighting during multisensory integration},
	volume = {15},
	copyright = {© 2011 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1097-6256},
	url = {http://www.nature.com/neuro/journal/v15/n1/full/nn.2983.html},
	doi = {10.1038/nn.2983},
	abstract = {Integration of multiple sensory cues is essential for precise and accurate perception and behavioral performance, yet the reliability of sensory signals can vary across modalities and viewing conditions. Human observers typically employ the optimal strategy of weighting each cue in proportion to its reliability, but the neural basis of this computation remains poorly understood. We trained monkeys to perform a heading discrimination task from visual and vestibular cues, varying cue reliability randomly. The monkeys appropriately placed greater weight on the more reliable cue, and population decoding of neural responses in the dorsal medial superior temporal area closely predicted behavioral cue weighting, including modest deviations from optimality. We found that the mathematical combination of visual and vestibular inputs by single neurons is generally consistent with recent theories of optimal probabilistic computation in neural circuits. These results provide direct evidence for a neural mechanism mediating a simple and widespread form of statistical inference.},
	language = {en},
	number = {1},
	urldate = {2013-06-28},
	journal = {Nature Neuroscience},
	author = {Fetsch, Christopher R. and Pouget, Alexandre and {DeAngelis}, Gregory C. and Angelaki, Dora E.},
	month = jan,
	year = {2012},
	keywords = {Behaviour, Computational neuroscience, Neurophysiology, Sensory systems},
	pages = {146--154},
	file = {Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/WT9RTVGB/Fetsch et al. - 2012 - Neural correlates of reliability-based cue weighti.pdf:application/pdf;Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/P8AWK6QU/nn.2983.html:text/html}
},

@inproceedings{bauer_som-based_2012,
	title = {A som-based model for multi-sensory integration in the superior colliculus},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6252816},
	urldate = {2013-05-26},
	booktitle = {Neural Networks ({IJCNN)}, The 2012 International Joint Conference on},
	author = {Bauer, Johannes and Weber, Cornelius and Wermter, Stefan},
	year = {2012},
	pages = {1–8},
	file = {06252816.pdf:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/DUFPFBS8/06252816.pdf:application/pdf}
},

@article{stein_multisensory_2008,
	title = {Multisensory integration: current issues from the perspective of the single neuron},
	volume = {9},
	shorttitle = {Multisensory integration},
	url = {http://www.nature.com/nrn/journal/v9/n4/abs/nrn2331.html},
	number = {4},
	urldate = {2013-05-26},
	journal = {Nature Reviews Neuroscience},
	author = {Stein, Barry E. and Stanford, Terrence R.},
	year = {2008},
	pages = {255–266},
	file = {2008stien.pdf:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/3J4CRVVK/2008stien.pdf:application/pdf}
},

@article{stanford_superadditivity_2007,
	title = {Superadditivity in multisensory integration: putting the computation in context},
	volume = {18},
	shorttitle = {Superadditivity in multisensory integration},
	url = {http://journals.lww.com/neuroreport/Abstract/2007/05280/Superadditivity_in_multisensory_integration_.13.aspx},
	number = {8},
	urldate = {2013-05-26},
	journal = {Neuroreport},
	author = {Stanford, Terrence R. and Stein, Barry E.},
	year = {2007},
	pages = {787–792},
	file = {stanford.2007.multisensory_review.pdf:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/33SST7PV/stanford.2007.multisensory_review.pdf:application/pdf}
},

@article{angelaki_multisensory_2009,
	title = {Multisensory integration: psychophysics, neurophysiology and computation},
	volume = {19},
	shorttitle = {Multisensory integration},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/pmc2749464/},
	number = {4},
	urldate = {2013-05-26},
	journal = {Current opinion in neurobiology},
	author = {Angelaki, Dora E. and Gu, Yong and {DeAngelis}, Gregory C.},
	year = {2009},
	pages = {452},
	file = {1-s2.0-S0959438809000725-main.pdf:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/U34W8TA3/1-s2.0-S0959438809000725-main.pdf:application/pdf}
},

@article{fetsch_visualvestibular_2010,
	title = {Visual–vestibular cue integration for heading perception: applications of optimal cue integration theory},
	volume = {31},
	shorttitle = {Visual–vestibular cue integration for heading perception},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1460-9568.2010.07207.x/full},
	number = {10},
	urldate = {2013-06-28},
	journal = {European Journal of Neuroscience},
	author = {Fetsch, Christopher R. and {DeAngelis}, Gregory C. and Angelaki, Dora E.},
	year = {2010},
	pages = {1721–1729},
	file = {j.1460-9568.2010.07207.x.pdf:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/X6PCQDMQ/j.1460-9568.2010.07207.x.pdf:application/pdf}
},

@article{rowland_model_2007,
	title = {A model of the neural mechanisms underlying multisensory integration in the superior colliculus},
	volume = {36},
	url = {http://www.perceptionweb.com/abstract.cgi?id=p5842},
	doi = {10.1068/p5842},
	abstract = {Much of the information about multisensory integration is derived from studies of the cat superior colliculus ({SC)}, a midbrain structure involved in orientation behaviors. This integration is apparent in the enhanced responses of {SC} neurons to cross-modal stimuli, responses that exceed those to any of the modality-specific component stimuli. The simplest model of multisensory integration is one in which the {SC} neuron simply sums its various sensory inputs. However, a number of empirical findings reveal the inadequacy of such a model; for example, the finding that deactivation of cortico-collicular inputs eliminates the enhanced response to a cross-modal stimulus without eliminating responses to the modality-specific component stimuli. These and other empirical findings inform a computational model that accounts for all of the most fundamental aspects of {SC} multisensory integration. The model is presented in two forms: an algebraic form that conveys the essential insights, and a compartmental form that represents the neuronal computations in a more biologically realistic way.},
	number = {10},
	urldate = {2013-06-28},
	journal = {Perception},
	author = {Rowland, Benjamin A. and Stanford, Terrence R. and Stein, Barry E.},
	year = {2007},
	pages = {1431 – 1443},
	file = {Perception Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/CR36HA3T/Rowland et al. - 2007 - A model of the neural mechanisms underlying multis.pdf:application/pdf;Perception Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/TBP8ZJMZ/abstract.html:text/html}
},

@article{alais_multisensory_2010,
	title = {Multisensory processing in review: from physiology to behaviour},
	volume = {23},
	shorttitle = {Multisensory processing in review},
	url = {http://www.ingentaconnect.com/content/brill/sp/2010/00000023/00000001/art00002},
	number = {1},
	urldate = {2013-05-26},
	journal = {Seeing and perceiving},
	author = {Alais, David and Newell, Fiona N. and Mamassian, Pascal},
	year = {2010},
	pages = {3–38},
	file = {Multisensory.pdf:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/3UEM8PTC/Multisensory.pdf:application/pdf}
},

@article{ohshiro_normalization_2011,
	title = {A normalization model of multisensory integration},
	volume = {14},
	copyright = {© 2011 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1097-6256},
	url = {http://www.nature.com/neuro/journal/v14/n6/abs/nn.2815.html},
	doi = {10.1038/nn.2815},
	abstract = {Responses of neurons that integrate multiple sensory inputs are traditionally characterized in terms of a set of empirical principles. However, a simple computational framework that accounts for these empirical features of multisensory integration has not been established. We propose that divisive normalization, acting at the stage of multisensory integration, can account for many of the empirical principles of multisensory integration shown by single neurons, such as the principle of inverse effectiveness and the spatial principle. This model, which uses a simple functional operation (normalization) for which there is considerable experimental support, also accounts for the recent observation that the mathematical rule by which multisensory neurons combine their inputs changes with cue reliability. The normalization model, which makes a strong testable prediction regarding cross-modal suppression, may therefore provide a simple unifying computational account of the important features of multisensory integration by neurons.
View full text},
	language = {en},
	number = {6},
	urldate = {2013-05-26},
	journal = {Nature Neuroscience},
	author = {Ohshiro, Tomokazu and Angelaki, Dora E. and {DeAngelis}, Gregory C.},
	year = {2011},
	pages = {775--782},
	file = {Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/F4MTDQPG/Ohshiro et al. - 2011 - A normalization model of multisensory integration.pdf:application/pdf;Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/BM9UR65M/nn.2815.html:text/html}
},

@article{fetsch_bridging_2013,
	title = {Bridging the gap between theories of sensory cue integration and the physiology of multisensory neurons},
	volume = {14},
	copyright = {© 2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1471-003x},
	url = {http://www.nature.com/nrn/journal/v14/n6/full/nrn3503.html#/key-points},
	doi = {10.1038/nrn3503},
	abstract = {The richness of perceptual experience, as well as its usefulness for guiding behaviour, depends on the synthesis of information across multiple senses. Recent decades have witnessed a surge in our understanding of how the brain combines sensory cues. Much of this research has been guided by one of two distinct approaches: one is driven primarily by neurophysiological observations, and the other is guided by principles of mathematical psychology and psychophysics. Conflicting results and interpretations have contributed to a conceptual gap between psychophysical and physiological accounts of cue integration, but recent studies of visual–vestibular cue integration have narrowed this gap considerably.},
	language = {en},
	number = {6},
	urldate = {2013-05-26},
	journal = {Nature Reviews Neuroscience},
	author = {Fetsch, Christopher R. and {DeAngelis}, Gregory C. and Angelaki, Dora E.},
	year = {2013},
	keywords = {Behaviour, Cognition, Computational neuroscience, Modelling, Nature Reviews Neuroscience},
	pages = {429--442},
	file = {Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/J4F5XUZG/Fetsch et al. - 2013 - Bridging the gap between theories of sensory cue i.pdf:application/pdf;Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/2B2JSW5M/nrn3503.html:text/html}
},

@article{zaidel_multisensory_2011,
	title = {Multisensory Calibration Is Independent of Cue Reliability},
	volume = {31},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/content/31/39/13949},
	doi = {10.1523/JNEUROSCI.2732-11.2011},
	abstract = {Multisensory calibration is fundamental for proficient interaction within a changing environment. Initial studies suggested a visual-dominant mechanism. More recently, a cue-reliability-based model, similar to optimal cue integration, has been proposed. However, a more general, reliability-independent model of fixed-ratio adaptation (of which visual dominance is a subcase) has never been tested. Here, we studied behavior of both humans and monkeys performing a heading-discrimination task. Subjects were presented with either visual (optic-flow), vestibular (motion-platform), or combined (visual–vestibular) stimuli and required to report whether self-motion was to the right/left of straight ahead. A systematic heading discrepancy was introduced between the visual and vestibular cues, without external feedback. Cue calibration was measured by the resulting sensory adaptation. Both visual and vestibular cues significantly adapted in the direction required to reduce cue conflict. However, unlike multisensory cue integration, cue calibration was not reliability based. Rather, a model of fixed-ratio adaptation best described the data, whereby vestibular adaptation was greater than visual adaptation, regardless of relative cue reliability. The average ratio of vestibular to visual adaptation was 1.75 and 2.30 for the human and monkey data, respectively. Furthermore, only through modeling fixed-ratio adaptation (using the ratio extracted from the data) were we able to account for reliability-based cue integration during the adaptation process. The finding that cue calibration does not depend on cue reliability is consistent with the notion that it follows an underlying estimate of cue accuracy. Cue accuracy is generally independent of cue reliability, and its estimate may change with a much slower time constant. Thus, greater vestibular versus visual (fixed-ratio) adaptation suggests lower vestibular versus visual cue accuracy.},
	language = {en},
	number = {39},
	urldate = {2013-05-27},
	journal = {The Journal of Neuroscience},
	author = {Zaidel, Adam and Turner, Amanda H. and Angelaki, Dora E.},
	month = sep,
	year = {2011},
	pages = {13949--13962},
	file = {13949.full.pdf:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/I7AZWCPQ/13949.full.pdf:application/pdf;Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/PHNXTZCT/Zaidel et al. - 2011 - Multisensory Calibration Is Independent of Cue Rel.pdf:application/pdf;Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/N53BWZ5D/13949.html:text/html}
},

@article{stanford_evaluating_2005,
	title = {Evaluating the Operations Underlying Multisensory Integration in the Cat Superior Colliculus},
	volume = {25},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/content/25/28/6499},
	doi = {10.1523/JNEUROSCI.5095-04.2005},
	abstract = {It is well established that superior colliculus ({SC)} multisensory neurons integrate cues from different senses; however, the mechanisms responsible for producing multisensory responses are poorly understood. Previous studies have shown that spatially congruent cues from different modalities (e.g., auditory and visual) yield enhanced responses and that the greatest relative enhancements occur for combinations of the least effective modality-specific stimuli. Although these phenomena are well documented, little is known about the mechanisms that underlie them, because no study has systematically examined the operation that multisensory neurons perform on their modality-specific inputs. The goal of this study was to evaluate the computations that multisensory neurons perform in combining the influences of stimuli from two modalities. The extracellular activities of single neurons in the {SC} of the cat were recorded in response to visual, auditory, and bimodal visual-auditory stimulation. Each neuron was tested across a range of stimulus intensities and multisensory responses evaluated against the null hypothesis of simple summation of unisensory influences. We found that the multisensory response could be superadditive, additive, or subadditive but that the computation was strongly dictated by the efficacies of the modality-specific stimulus components. Superadditivity was most common within a restricted range of near-threshold stimulus efficacies, whereas for the majority of stimuli, response magnitudes were consistent with the linear summation of modality-specific influences. In addition to providing a constraint for developing models of multisensory integration, the relationship between response mode and stimulus efficacy emphasizes the importance of considering stimulus parameters when inducing or interpreting multisensory phenomena.},
	language = {en},
	number = {28},
	urldate = {2013-05-27},
	journal = {The Journal of Neuroscience},
	author = {Stanford, Terrence R. and Quessy, Stephan and Stein, Barry E.},
	month = jul,
	year = {2005},
	note = {{PMID:} 16014711},
	keywords = {audition, cat, multisensory integration, single-unit electrophysiology, superior colliculus, vision},
	pages = {6499--6508},
	file = {Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/GG939RHC/Stanford et al. - 2005 - Evaluating the Operations Underlying Multisensory .pdf:application/pdf;Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/FR5HB2S5/6499.html:text/html}
},

@book{trommershauser_sensory_2011,
	title = {Sensory cue integration},
	isbn = {9780195387247},
	abstract = {This book provides an introduction into both computational models and experimental paradigms that are concerned with sensory cue integration both within and between sensory modalities. Importantly, across behavioral, electrophysiological and theoretical approaches, Bayesian statistics is emerging as a common language in which cue-combination problems can be expressed. This book focuses on the emerging probabilistic way of thinking about these problems. These approaches derive from the realization that all our sensors are noisy and moreover are often affected by ambiguity. For example, mechanoreceptor outputs are variable and they cannot distinguish if a perceived force is caused by the weight of an object or by force we are producing ourselves. The computational approaches described in this book aim at formalizing the uncertainty of cues. They describe cue combination as the nervous system's attempt to minimize uncertainty in its estimates and to choose successful actions. Some computational approaches described in the chapters of this book are concerned with the application of such statistical ideas to real-world cue-combination problems, such as shape and depth perception. Other parts of the book ask how uncertainty may be represented in the nervous system and used for cue combination.  The broadening scope of probabilistic approaches to cue combination is highlighted in the breadth of topics covered in this book: the chapters summarize and discuss computational approaches and behavioral evidence aimed at understanding the combination of visual, auditory, proprioceptive, and haptic cues. Some chapters address the combination of cues within a single sensory modality while others address the combination across sensory modalities. Neural implementation, behavior, and theory are considered. The unifying aspect of this book is the focus on the uncertainty intrinsic to sensory cues and the underlying question of how the nervous system deals with this uncertainty.  The book is intended as a reference text for graduate students and professionals in perceptual psychology, computational neuroscience, cognitive neuroscience and sensory neurophysiology.},
	language = {en},
	publisher = {Oxford University Press},
	author = {Trommershäuser, Julia and Körding, Konrad P. and Landy, Michael S.},
	year = {2011},
	keywords = {Medical / Neurology, Medical / Neuroscience, Psychology / Cognitive Psychology}
},

@article{alvarado_neural_2008,
	title = {A neural network model of multisensory integration also accounts for unisensory integration in superior colliculus},
	volume = {1242},
	issn = {0006-8993},
	shorttitle = {Multisensory Integration},
	url = {http://www.sciencedirect.com/science/article/pii/S0006899308008032},
	doi = {10.1016/j.brainres.2008.03.074},
	abstract = {Sensory integration is a characteristic feature of superior colliculus ({SC)} neurons. A recent neural network model of single-neuron integration derived a set of basic biological constraints sufficient to replicate a number of physiological findings pertaining to multisensory responses. The present study examined the accuracy of this model in predicting the responses of {SC} neurons to pairs of visual stimuli placed within their receptive fields. The accuracy of this model was compared to that of three other computational models (additive, averaging and maximum operator) previously used to fit these data. Each neuron's behavior was assessed by examining its mean responses to the component stimuli individually and together, and each model's performance was assessed to determine how close its prediction came to the actual mean response of each neuron and the magnitude of its predicted residual error. Predictions from the additive model significantly overshot the actual responses of {SC} neurons and predictions from the averaging model significantly undershot them. Only the predictions of the maximum operator and neural network model were not significantly different from the actual responses. However, the neural network model outperformed even the maximum operator model in predicting the responses of these neurons. The neural network model is derived from a larger model that also has substantial predictive power in multisensory integration, and provides a single computational vehicle for assessing the responses of {SC} neurons to different combinations of cross-modal and within-modal stimuli of different efficacies.},
	urldate = {2013-06-28},
	journal = {Brain Research},
	author = {Alvarado, Juan Carlos and Rowland, Benjamin A. and Stanford, Terrence R. and Stein, Barry E.},
	month = nov,
	year = {2008},
	keywords = {Averaging, Computation, Maximum, Multisensory, Within-modal},
	pages = {13--23},
	file = {ScienceDirect Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/8RPATK83/Alvarado et al. - 2008 - A neural network model of multisensory integration.pdf:application/pdf;ScienceDirect Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/KFXCHIHD/S0006899308008032.html:text/html}
},

@article{ernst_humans_2002,
	title = {Humans integrate visual and haptic information in a statistically optimal fashion},
	volume = {415},
	copyright = {© 2002 Nature Publishing Group},
	issn = {0028-0836},
	url = {http://www.nature.com/nature/journal/v415/n6870/abs/415429a.html},
	doi = {10.1038/415429a},
	abstract = {When a person looks at an object while exploring it with their hand, vision and touch both provide information for estimating the properties of the object. Vision frequently dominates the integrated visual–haptic percept, for example when judging size, shape or position, but in some circumstances the percept is clearly affected by haptics. Here we propose that a general principle, which minimizes variance in the final estimate, determines the degree to which vision or haptics dominates. This principle is realized by using maximum-likelihood estimation to combine the inputs. To investigate cue combination quantitatively, we first measured the variances associated with visual and haptic estimation of height. We then used these measurements to construct a maximum-likelihood integrator. This model behaved very similarly to humans in a visual–haptic task. Thus, the nervous system seems to combine visual and haptic information in a fashion that is similar to a maximum-likelihood integrator. Visual dominance occurs when the variance associated with visual estimation is lower than that associated with haptic estimation.},
	language = {en},
	number = {6870},
	urldate = {2013-05-28},
	journal = {Nature},
	author = {Ernst, Marc O. and Banks, Martin S.},
	year = {2002},
	keywords = {astronomy, astrophysics, biochemistry, bioinformatics, biology, biotechnology, cancer, cell cycle, cell signalling, climate change, computational biology, development, developmental biology, {DNA}, drug discovery, earth science, ecology, environmental science, evolution, evolutionary biology, functional genomics, genetics, genomics, geophysics, immunology, interdisciplinary science, life, marine biology, materials science, medical research, medicine, metabolomics, Molecular biology, molecular interactions, nanotechnology, Nature, neurobiology, neuroscience, palaeobiology, pharmacology, physics, proteomics, quantum physics, {RNA}, Science, science news, science policy, signal transduction, structural biology, systems biology, transcriptomics},
	pages = {429--433},
	file = {Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/STR6BQSK/Ernst  Banks - 2002 - Humans integrate visual and haptic information in .pdf:application/pdf;Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/BF39Q46X/415429a.html:text/html}
},

@article{ma_bayesian_2006,
	title = {Bayesian inference with probabilistic population codes},
	volume = {9},
	copyright = {© 2006 Nature Publishing Group},
	issn = {1097-6256},
	url = {http://www.nature.com/neuro/journal/v9/n11/full/nn1790.html},
	doi = {10.1038/nn1790},
	abstract = {Recent psychophysical experiments indicate that humans perform near-optimal Bayesian inference in a wide variety of tasks, ranging from cue integration to decision making to motor control. This implies that neurons both represent probability distributions and combine those distributions according to a close approximation to Bayes' rule. At first sight, it would seem that the high variability in the responses of cortical neurons would make it difficult to implement such optimal statistical inference in cortical circuits. We argue that, in fact, this variability implies that populations of neurons automatically represent probability distributions over the stimulus, a type of code we call probabilistic population codes. Moreover, we demonstrate that the Poisson-like variability observed in cortex reduces a broad class of Bayesian inference to simple linear combinations of populations of neural activity. These results hold for arbitrary probability distributions over the stimulus, for tuning curves of arbitrary shape and for realistic neuronal variability.},
	language = {en},
	number = {11},
	urldate = {2013-05-27},
	journal = {Nature Neuroscience},
	author = {Ma, Wei Ji and Beck, Jeffrey M. and Latham, Peter E. and Pouget, Alexandre},
	year = {2006},
	pages = {1432--1438},
	file = {Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/CQUMK72G/Ma et al. - 2006 - Bayesian inference with probabilistic population c.pdf:application/pdf;Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/TWM728I8/nn1790.html:text/html}
},

@article{weisswange_bayesian_2011,
	title = {Bayesian Cue Integration as a Developmental Outcome of Reward Mediated Learning},
	volume = {6},
	url = {http://dx.doi.org/10.1371/journal.pone.0021575},
	doi = {10.1371/journal.pone.0021575},
	abstract = {Average human behavior in cue combination tasks is well predicted by Bayesian inference models. As this capability is acquired over developmental timescales, the question arises, how it is learned. Here we investigated whether reward dependent learning, that is well established at the computational, behavioral, and neuronal levels, could contribute to this development. It is shown that a model free reinforcement learning algorithm can indeed learn to do cue integration, i.e. weight uncertain cues according to their respective reliabilities and even do so if reliabilities are changing. We also consider the case of causal inference where multimodal signals can originate from one or multiple separate objects and should not always be integrated. In this case, the learner is shown to develop a behavior that is closest to Bayesian model averaging. We conclude that reward mediated learning could be a driving force for the development of cue integration and causal inference.},
	number = {7},
	urldate = {2013-05-27},
	journal = {{PLoS} {ONE}},
	author = {Weisswange, Thomas H. and Rothkopf, Constantin A. and Rodemann, Tobias and Triesch, Jochen},
	year = {2011},
	pages = {e21575},
	file = {PLoS Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/BZKPUJ7M/Weisswange et al. - 2011 - Bayesian Cue Integration as a Developmental Outcom.pdf:application/pdf}
},

@article{fetsch_dynamic_2009,
	title = {Dynamic Reweighting of Visual and Vestibular Cues during Self-Motion Perception},
	volume = {29},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/content/29/49/15601},
	doi = {10.1523/JNEUROSCI.2574-09.2009},
	abstract = {The perception of self-motion direction, or heading, relies on integration of multiple sensory cues, especially from the visual and vestibular systems. However, the reliability of sensory information can vary rapidly and unpredictably, and it remains unclear how the brain integrates multiple sensory signals given this dynamic uncertainty. Human psychophysical studies have shown that observers combine cues by weighting them in proportion to their reliability, consistent with statistically optimal integration schemes derived from Bayesian probability theory. Remarkably, because cue reliability is varied randomly across trials, the perceptual weight assigned to each cue must change from trial to trial. Dynamic cue reweighting has not been examined for combinations of visual and vestibular cues, nor has the Bayesian cue integration approach been applied to laboratory animals, an important step toward understanding the neural basis of cue integration. To address these issues, we tested human and monkey subjects in a heading discrimination task involving visual (optic flow) and vestibular (translational motion) cues. The cues were placed in conflict on a subset of trials, and their relative reliability was varied to assess the weights that subjects gave to each cue in their heading judgments. We found that monkeys can rapidly reweight visual and vestibular cues according to their reliability, the first such demonstration in a nonhuman species. However, some monkeys and humans tended to over-weight vestibular cues, inconsistent with simple predictions of a Bayesian model. Nonetheless, our findings establish a robust model system for studying the neural mechanisms of dynamic cue reweighting in multisensory perception.},
	language = {en},
	number = {49},
	urldate = {2013-05-28},
	journal = {The Journal of Neuroscience},
	author = {Fetsch, Christopher R. and Turner, Amanda H. and {DeAngelis}, Gregory C. and Angelaki, Dora E.},
	month = dec,
	year = {2009},
	pages = {15601--15612},
	file = {Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/GVS6B7MK/Fetsch et al. - 2009 - Dynamic Reweighting of Visual and Vestibular Cues .pdf:application/pdf;Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/EG3ZXXR3/15601.html:text/html}
},

@article{geisler_contributions_2011,
	title = {Contributions of ideal observer theory to vision research},
	volume = {51},
	issn = {0042-6989},
	shorttitle = {Vision Research 50th Anniversary Issue: Part 1},
	url = {http://www.sciencedirect.com/science/article/pii/S0042698910004724},
	doi = {10.1016/j.visres.2010.09.027},
	abstract = {An ideal observer is a hypothetical device that performs optimally in a perceptual task given the available information. The theory of ideal observers has proven to be a powerful and useful tool in vision research, which has been applied to a wide range of problems. Here I first summarize the basic concepts and logic of ideal observer analysis and then briefly describe applications in a number of different areas, including pattern detection, discrimination and estimation, perceptual grouping, shape, depth and motion perception and visual attention, with an emphasis on recent applications. Given recent advances in mathematical statistics, in computational power, and in techniques for measuring behavioral performance, neural activity and natural scene statistics, it seems certain that ideal observer theory will play an ever increasing role in basic and applied areas of vision science.},
	number = {7},
	urldate = {2013-05-28},
	journal = {Vision Research},
	author = {Geisler, Wilson S.},
	month = apr,
	year = {2011},
	keywords = {Ideal observer theory, Natural scene statistics, Natural tasks, Psychophysics},
	pages = {771--781},
	file = {ScienceDirect Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/SWN9EZK3/Geisler - 2011 - Contributions of ideal observer theory to vision r.pdf:application/pdf;ScienceDirect Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/D8WMD8FT/S0042698910004724.html:text/html}
},

@article{knill_bayesian_2004,
	title = {The Bayesian brain: the role of uncertainty in neural coding and computation},
	volume = {27},
	issn = {0166-2236},
	shorttitle = {The Bayesian brain},
	url = {http://www.sciencedirect.com/science/article/pii/S0166223604003352},
	doi = {10.1016/j.tins.2004.10.007},
	abstract = {To use sensory information efficiently to make judgments and guide action in the world, the brain must represent and use information about uncertainty in its computations for perception and action. Bayesian methods have proven successful in building computational theories for perception and sensorimotor control, and psychophysics is providing a growing body of evidence that human perceptual computations are {‘Bayes'} optimal’. This leads to the {‘Bayesian} coding hypothesis’: that the brain represents sensory information probabilistically, in the form of probability distributions. Several computational schemes have recently been proposed for how this might be achieved in populations of neurons. Neurophysiological data on the hypothesis, however, is almost non-existent. A major challenge for neuroscientists is to test these ideas experimentally, and so determine whether and how neurons code information about sensory uncertainty.},
	number = {12},
	urldate = {2013-05-29},
	journal = {Trends in Neurosciences},
	author = {Knill, David C. and Pouget, Alexandre},
	year = {2004},
	pages = {712--719},
	file = {ScienceDirect Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/B4NHT3QQ/Knill  Pouget - 2004 - The Bayesian brain the role of uncertainty in neu.pdf:application/pdf;ScienceDirect Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/4IW3ZJVC/S0166223604003352.html:text/html}
},

@article{crowell_ideal_1996,
	title = {Ideal observer for heading judgments},
	volume = {36},
	issn = {0042-6989},
	url = {http://www.sciencedirect.com/science/article/pii/0042698995001212},
	doi = {10.1016/0042-6989(95)00121-2},
	abstract = {Several aspects of the viewing situation affect the ability to determine heading from optical flow. These include the amount of depth variation and number of texture elements in the scene, the location and amount of the visual field stimulated, and the position of the focus of expansion within the stimulus. Without a quantification of the discrimination information provided by the stimuli presented to the observer, it is impossible to determine how much of an observed change in performance reflects the properties of neural mechanisms and strategies employed by the observer. To enable a better quantification, we developed an ideal observer for the discrimination of heading from random-dot flow fields. Internal noises of the ideal observer were set by the results of single-dot velocity discrimination experiments. We compared human and ideal observer performance in discriminating headings with different patterns of flow (e.g. radial vs laminar) presented on different parts of the retina. Efficiency—the ratio of ideal and human thresholds—was fairly constant for the various flow patterns and retinal eccentricities. This outcome indicates that most of the variation in human observers' ability to estimate heading from the flow patterns and retinal loci considered here is due to changes in the discrimination information provided by the stimulus after measurement by the visual system. In the discussion, we show how the ideal observer can be used to quantify the spatial distribution of heading discrimination information for any observer translation through any scene represented by dots.},
	number = {3},
	urldate = {2013-05-29},
	journal = {Vision Research},
	author = {Crowell, James A. and Banks, Martin S.},
	year = {1996},
	keywords = {Heading, Ideal observer, Motion, Optic flow, Signal detection},
	pages = {471--490},
	file = {ScienceDirect Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/RVHCGGF8/Crowell  Banks - 1996 - Ideal observer for heading judgments.pdf:application/pdf;ScienceDirect Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/87PWSICG/0042698995001212.html:text/html}
},

@article{liu_object_1995,
	title = {Object classification for human and ideal observers},
	volume = {35},
	issn = {0042-6989},
	url = {http://www.sciencedirect.com/science/article/pii/004269899400150K},
	doi = {10.1016/0042-6989(94)00150-K},
	abstract = {We describe a novel approach, based on ideal observer analysis, for measuring the ability of human observers to use image information for {3D} object perception. We compute the statistical efficiency of subjects relative to an ideal observer for a {3D} object classification task. After training to 11 different views of a randomly shaped thick wire object, subjects were asked which of a pair of noisy views of the object best matched the learned object. Efficiency relative to the actual information in the stimuli can be as high as 20\%. Increases in object regularity (e.g. symmetry) lead to increases in the efficiency with which novel views of an object could be classified. Furthermore, such increases in regularity also lead to decreases in the effect of viewpoint on classification efficiency. Human statistical efficiencies relative to a {2D} ideal observer exceeded 100\%, thereby excluding all models which are sub-optimal relative to the {2D} ideal.},
	number = {4},
	urldate = {2013-05-29},
	journal = {Vision Research},
	author = {Liu, Zili and Knill, David C. and Kersten, Daniel},
	year = {1995},
	keywords = {Ideal observers, Object recognition, Radial basis functions, Template matching},
	pages = {549--568},
	file = {ScienceDirect Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/QVHD5CG5/Liu et al. - 1995 - Object classification for human and ideal observer.pdf:application/pdf;ScienceDirect Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/P7FB6CTA/004269899400150K.html:text/html}
},

@article{alais_ventriloquist_2004,
	title = {The Ventriloquist Effect Results from Near-Optimal Bimodal Integration},
	volume = {14},
	issn = {0960-9822},
	url = {http://www.sciencedirect.com/science/article/pii/S0960982204000430},
	doi = {10.1016/j.cub.2004.01.029},
	abstract = {Ventriloquism is the ancient art of making one's voice appear to come from elsewhere, an art exploited by the Greek and Roman oracles, and possibly earlier [1]. We regularly experience the effect when watching television and movies, where the voices seem to emanate from the actors' lips rather than from the actual sound source. Originally, ventriloquism was explained by performers projecting sound to their puppets by special techniques [1], but more recently it is assumed that ventriloquism results from vision “capturing” sound [2–5]. In this study we investigate spatial localization of audio-visual stimuli. When visual localization is good, vision does indeed dominate and capture sound. However, for severely blurred visual stimuli (that are poorly localized), the reverse holds: sound captures vision. For less blurred stimuli, neither sense dominates and perception follows the mean position. Precision of bimodal localization is usually better than either the visual or the auditory unimodal presentation. All the results are well explained not by one sense capturing the other, but by a simple model of optimal combination of visual and auditory information.},
	number = {3},
	urldate = {2013-05-29},
	journal = {Current Biology},
	author = {Alais, David and Burr, David},
	year = {2004},
	pages = {257--262},
	file = {ScienceDirect Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/IW4G5RC7/Alais  Burr - 2004 - The Ventriloquist Effect Results from Near-Optimal.pdf:application/pdf;ScienceDirect Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/Z7PCPZ82/S0960982204000430.html:text/html}
},

@article{kording_causal_2007,
	title = {Causal Inference in Multisensory Perception},
	volume = {2},
	url = {http://dx.plos.org/10.1371/journal.pone.0000943},
	doi = {10.1371/journal.pone.0000943},
	abstract = {Perceptual events derive their significance to an animal from their meaning about the world, that is from the information they carry about their causes. The brain should thus be able to efficiently infer the causes underlying our sensory events. Here we use multisensory cue combination to study causal inference in perception. We formulate an ideal-observer model that infers whether two sensory cues originate from the same location and that also estimates their location(s). This model accurately predicts the nonlinear integration of cues by human subjects in two auditory-visual localization tasks. The results show that indeed humans can efficiently infer the causal structure as well as the location of causes. By combining insights from the study of causal inference with the ideal-observer approach to sensory cue combination, we show that the capacity to infer causal structure is not limited to conscious, high-level cognition; it is also performed continually and effortlessly in perception.},
	number = {9},
	urldate = {2013-05-29},
	journal = {{PLoS} {ONE}},
	author = {Körding, Konrad P. and Beierholm, Ulrik and Ma, Wei Ji and Quartz, Steven and Tenenbaum, Joshua B. and Shams, Ladan},
	year = {2007},
	pages = {e943},
	file = {PLoS Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/6EF5AAFM/Körding et al. - 2007 - Causal Inference in Multisensory Perception.pdf:application/pdf;PLoS Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/D4K9CHQT/infodoi10.1371journal.pone.html:text/html}
},

@article{meredith_interactions_1983,
	title = {Interactions among converging sensory inputs in the superior colliculus},
	volume = {221},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/content/221/4608/389},
	doi = {10.1126/science.6867718},
	abstract = {The responses of superior colliculus cells to a given sensory stimulus were influenced by the presence or absence of other sensory cues. By pooling sensory inputs, many superior colliculus cells seem to amplify the effects of subtle environmental cues in certain conditions, whereas in others, responses to normally effective stimuli can be blocked. The observations illustrate the dynamic, interactive nature of the multisensory inputs which characterize the deeper laminae of the superior colliculus.},
	language = {en},
	number = {4608},
	urldate = {2013-05-29},
	journal = {Science},
	author = {Meredith, M. A. and Stein, B. E.},
	month = jul,
	year = {1983},
	pages = {389--391},
	file = {Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/S44AZ4C4/389.html:text/html}
},

@article{meredith_visual_1986,
	title = {Visual, auditory, and somatosensory convergence on cells in superior colliculus results in multisensory integration},
	volume = {56},
	issn = {0022-3077, 1522-1598},
	url = {http://jn.physiology.org/content/56/3/640},
	abstract = {Convergence of inputs from different sensory modalities onto individual neurons is a phenomenon that occurs widely throughout the brain at many phyletic levels and appears to represent a basic neural mechanism by which an organism integrates complex environmental stimuli. In the present study, neurons in the superior colliculus ({SC)} were used as a model to examine how single neurons deal with simultaneous cues from different sensory modalities (e.g., visual, auditory, somatosensory). The functional result of multisensory convergence on an individual cell was determined by comparing the responses evoked from it by a combined-modality (multimodal) stimulus with those elicited by each (unimodal) component of that stimulus presented alone. Superior colliculus cells exhibited profound changes in their activity when individual sensory stimuli were combined. These "multisensory interactions" were found to be widespread among deep laminae cells and fell into one of two functional categories: response enhancement, characterized by a significant increase in the number of discharges evoked; and response depression, characterized by a significant decrease in the discharges elicited. Multisensory response interactions most often reflected a multiplicative, rather than summative, change in activity. Their absolute magnitude varied from cell to cell and, when stimulus conditions were altered, within the same cell. However, the percentage change of enhanced interactions was generally inversely related to the vigor of the responses that could be evoked by presenting each unimodal stimulus alone and suggest that the potential for response amplification was greatest when responses evoked by individual stimuli were weakest. The majority of cells exhibiting multi-sensory characteristics were demonstrated to have descending efferent projections and thus had access to premotor and motor areas of the brain stem and spinal cord involved in {SC-mediated} attentive and orientation behaviors. These data show that multisensory convergence provides the descending efferent cells of the {SC} with a dynamic response character. The responses of these cells and the {SC-mediated} behaviors that they underlie need not be immutably tied to the presence of any single stimulus, but can vary in response to the particular complex of stimuli present in the environment at any given moment.},
	language = {en},
	number = {3},
	urldate = {2013-05-29},
	journal = {Journal of Neurophysiology},
	author = {Meredith, M. A. and Stein, B. E.},
	month = sep,
	year = {1986},
	pages = {640--662},
	file = {Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/UTPTVM8P/Meredith  Stein - 1986 - Visual, auditory, and somatosensory convergence on.pdf:application/pdf;Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/44G7V6JE/640.html:text/html}
},

@article{sparks_translation_1986,
	title = {Translation of sensory signals into commands for control of saccadic eye movements: role of primate superior colliculus},
	volume = {66},
	issn = {0031-9333, 1522-1210},
	shorttitle = {Translation of sensory signals into commands for control of saccadic eye movements},
	url = {http://physrev.physiology.org/content/66/1/118},
	abstract = {Afferent signals that guide orienting movements converge in the deeper layers of the {SC} in a wide variety of animals. The sensory cells are arranged topographically according to their receptive-field locations and, thereby, form maps of sensory space. Maps of visual, somatosensory, and/or auditory space have been obtained in the iguana, mouse, hamster, barn owl, chinchilla, cat, and monkey. The deeper layers of the {SC} also contain neurons involved in the generation of movements of the eyes, head, vibrissae, and pinnae. Thus the {SC}, a site containing multiple sensory maps and perhaps multiple motor maps, has been selected by many investigators as a structure for investigating the problem of sensorimotor integration. In the mammalian nervous system, emphasized in this review, much remains to be learned about the structure, organization, and function of the {SC.} While anatomical studies continue to add to the knowledge of the sources of afferent projections, their pattern of laminar termination, and the source and destination of efferent projections, relatively little is known about the intrinsic organization of the colliculus, especially the deeper layers. Recently, electrophysiological studies have moved from an emphasis on the sensory and motor properties of collicular neurons to an examination of the maps of auditory and somatosensory space and the correspondence of these maps. In the future, major efforts aimed at identifying the functional properties of cells that project to the {SC} from diverse brain regions as well as the functional properties that project to the various structures receiving input from the colliculus are needed. A combination of anatomical and electrophysiological methods is required to describe the signal transforms that occur between the {SC} and motor areas (such as the paramedian pontine reticular formation) closer to the final common pathway. Conceptual and empirical work is needed to develop and test models of how the dynamic visual and auditory maps found in the primate {SC} are generated. In general, new and/or improved models of the role of the {SC} in sensorimotor integration are needed as guides for future research. A point of view emphasized here is that it may be fruitful to examine the function of the {SC} from a motor perspective. The nature of the motor command imposes constraints on the configuration of signals that can initiate movements and thereby determines the required transformation of sensory signals.},
	language = {en},
	number = {1},
	urldate = {2013-05-29},
	journal = {Physiological Reviews},
	author = {Sparks, D. L.},
	month = jan,
	year = {1986},
	pages = {118--171},
	file = {Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/IT9MQ2AE/Sparks - 1986 - Translation of sensory signals into commands for c.pdf:application/pdf;Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/GT68H9BH/118.html:text/html}
},

@article{gu_visual_2006,
	title = {Visual and Nonvisual Contributions to Three-Dimensional Heading Selectivity in the Medial Superior Temporal Area},
	volume = {26},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/content/26/1/73},
	doi = {10.1523/JNEUROSCI.2356-05.2006},
	abstract = {Robust perception of self-motion requires integration of visual motion signals with nonvisual cues. Neurons in the dorsal subdivision of the medial superior temporal area ({MSTd)} may be involved in this sensory integration, because they respond selectively to global patterns of optic flow, as well as translational motion in darkness. Using a virtual-reality system, we have characterized the three-dimensional ({3D)} tuning of {MSTd} neurons to heading directions defined by optic flow alone, inertial motion alone, and congruent combinations of the two cues. Among 255 {MSTd} neurons, 98\% exhibited significant {3D} heading tuning in response to optic flow, whereas 64\% were selective for heading defined by inertial motion. Heading preferences for visual and inertial motion could be aligned but were just as frequently opposite. Moreover, heading selectivity in response to congruent visual/vestibular stimulation was typically weaker than that obtained using optic flow alone, and heading preferences under congruent stimulation were dominated by the visual input. Thus, {MSTd} neurons generally did not integrate visual and nonvisual cues to achieve better heading selectivity. A simple two-layer neural network, which received eye-centered visual inputs and head-centered vestibular inputs, reproduced the major features of the {MSTd} data. The network was trained to compute heading in a head-centered reference frame under all stimulus conditions, such that it performed a selective reference-frame transformation of visual, but not vestibular, signals. The similarity between network hidden units and {MSTd} neurons suggests that {MSTd} may be an early stage of sensory convergence involved in transforming optic flow information into a (head-centered) reference frame that facilitates integration with vestibular signals.},
	language = {en},
	number = {1},
	urldate = {2013-05-29},
	journal = {The Journal of Neuroscience},
	author = {Gu, Yong and Watkins, Paul V. and Angelaki, Dora E. and {DeAngelis}, Gregory C.},
	month = jan,
	year = {2006},
	keywords = {Heading, monkey, {MST}, Optic flow, vestibular, visual},
	pages = {73--85},
	file = {Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/BHC8JVPF/Gu et al. - 2006 - Visual and Nonvisual Contributions to Three-Dimens.pdf:application/pdf;Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/7KNR86GS/73.html:text/html}
},

@article{morgan_multisensory_2008,
	title = {Multisensory integration in macaque visual cortex depends on cue reliability},
	volume = {59},
	issn = {0896-6273},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2601653/},
	doi = {10.1016/j.neuron.2008.06.024},
	abstract = {Responses of multisensory neurons to combinations of sensory cues are generally enhanced or depressed relative to single cues presented alone. We examined integration of visual and vestibular self-motion cues in area {MSTd.} We presented unimodal as well as congruent and conflicting bimodal stimuli across a full range of headings in the horizontal plane. This broad exploration of stimulus space permitted evaluation of hypothetical combination rules employed by multisensory neurons. Bimodal responses were well fit by weighted linear sums of unimodal responses, with weights typically less than one (sub-additive). Importantly, weights change with the relative reliabilities of the two cues: visual weights decrease and vestibular weights increase when visual stimuli are degraded. Moreover, both modulation depth and neuronal discrimination thresholds improve for matched bimodal as compared to unimodal stimuli, which might allow for increased neural sensitivity during multisensory stimulation. These findings establish important new constraints for neural models of cue integration.},
	number = {4},
	urldate = {2013-06-25},
	journal = {Neuron},
	author = {Morgan, Michael L. and {DeAngelis}, Gregory C. and Angelaki, Dora E.},
	month = aug,
	year = {2008},
	pages = {662--673},
	file = {MorganDeAngelisAngeliki08_Neuron.pdf:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/BFNK52D6/MorganDeAngelisAngeliki08_Neuron.pdf:application/pdf;PubMed Central Full Text PDF:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/NSKJPGRD/Morgan et al. - 2008 - Multisensory integration in macaque visual cortex .pdf:application/pdf}
},

@article{bremmer_visualvestibular_2002,
	title = {Visual–vestibular interactive responses in the macaque ventral intraparietal area ({VIP)}},
	volume = {16},
	issn = {1460-9568},
	url = {http://onlinelibrary.wiley.com/doi/10.1046/j.1460-9568.2002.02206.x/abstract},
	doi = {10.1046/j.1460-9568.2002.02206.x},
	abstract = {Self-motion detection requires the interaction of a number of sensory systems for correct perceptual interpretation of a given movement and an eventual motor response. Parietal cortical areas are thought to play an important role in this function, and we have thus studied the encoding of multimodal signals and their spatiotemporal interactions in the ventral intraparietal area of macaque monkeys. Thereby, we have identified for the first time the presence of vestibular sensory input to this area and described its interaction with somatosensory and visual signals, via extracellular single-cell recordings in awake head-fixed animals. Visual responses were driven by large field stimuli that simulated either backward or forward self-motion (contraction or expansion stimuli, respectively), or movement in the frontoparallel plane (visual increments moving simultaneously in the same direction). While the dominant sensory modality in most neurons was visual, about one third of all recorded neurons responded to horizontal rotation. These vestibular responses were typically in phase with head velocity, but in some cases they could signal acceleration or even showed integration to position. The associated visual responses were always codirectional with the vestibular on-direction, i.e. noncomplementary. Somatosensory responses were in register with the visual preferred direction, either in the same or in the opposite direction, thus signalling translation or rotation in the horizontal plane. These results, taken together with data on responses to optic flow stimuli obtained in a parallel study, strongly suggest an involvement of area {VIP} in the analysis and the encoding of self-motion.},
	language = {en},
	number = {8},
	urldate = {2013-06-26},
	journal = {European Journal of Neuroscience},
	author = {Bremmer, Frank and Klam, François and Duhamel, Jean-René and Ben Hamed, Suliann and Graf, Werner},
	year = {2002},
	keywords = {parietal, perception, primate, self-motion, vestibular},
	pages = {1569–1586},
	file = {Bremmer et al. - 2002 - Visual–vestibular interactive responses in the mac.pdf:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/EGB38Z3M/Bremmer et al. - 2002 - Visual-vestibular interactive responses in the mac.pdf:application/pdf;Snapshot:/home/hwp/.mozilla/firefox/8scdd80y.default/zotero/storage/WJEPZWP6/full.html:text/html}
}